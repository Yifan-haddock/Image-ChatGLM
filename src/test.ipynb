{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BeitModel, BeitImageProcessor\n",
    "from PIL import Image\n",
    "import datasets\n",
    "from modeling_chatglm import ChatGLMForConditionalGeneration, ChatGLMConfig\n",
    "from tokenization_chatglm import ChatGLMTokenizer\n",
    "from peft import (get_peft_model_state_dict, get_peft_model, LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modeling_chatglm' from '/Share/home/qiyifan/filebase/projects/multi-modal/src/modeling_chatglm.py'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modeling_chatglm\n",
    "importlib.reload(modeling_chatglm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_MASK = 64789\n",
    "ID_gMASK = 64790\n",
    "ID_sMASK = 64791\n",
    "ID_SOP = 64792\n",
    "ID_EOP = 64793\n",
    "ID_BOS = 1\n",
    "ID_EOS = 2\n",
    "ID_PAD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc39b33efdad4a978f56b836a232bbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /Share/home/qiyifan/filebase/source/chatglm2-6b and are newly initialized: ['transformer.image_encoder.align.image_h_to_kv.bias', 'transformer.image_encoder.align.image_h_to_kv.weight', 'transformer.image_encoder.align.image_e_to_h.bias', 'transformer.image_encoder.align.image_e_to_h.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PATH_MODEL_PRETRAIN='/Share/home/qiyifan/filebase/source/chatglm2-6b'\n",
    "ImageEncoderPath=\"/Share/home/qiyifan/filebase/source/beit-base-patch16-224-pt22k-ft22k\"\n",
    "tokenizer = ChatGLMTokenizer.from_pretrained(PATH_MODEL_PRETRAIN)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "model = modeling_chatglm.ChatGLMForConditionalGeneration.from_pretrained(PATH_MODEL_PRETRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Share/home/qiyifan/filebase/source/beit-base-patch16-224-pt22k-ft22k were not used when initializing BeitModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BeitModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BeitModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model.transformer.init_image_model(\"/Share/home/qiyifan/filebase/source/beit-base-patch16-224-pt22k-ft22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_modules(model):\n",
    "    # Initialize a Set to Store Unique Layers\n",
    "    unique_layers = set()\n",
    "    \n",
    "    # Iterate Over All Named Modules in the Model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the Module Type Contains 'Linear4bit'\n",
    "        if \"Linear\" in str(type(module)) and \"image\" not in name:\n",
    "            # Extract the Type of the Layer\n",
    "            layer_type = name.split('.')[-1]\n",
    "            \n",
    "            # Add the Layer Type to the Set of Unique Layers\n",
    "            unique_layers.add(name)\n",
    "\n",
    "    # Return the Set of Unique Layers Converted to a List\n",
    "    return list(unique_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_MODULES = find_target_modules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(target_modules=TARGET_MODULES,\n",
    "                    lora_dropout=0.05,\n",
    "                    lora_alpha=16,\n",
    "                    task_type=\"CAUSAL_LM\",\n",
    "                    bias=\"none\",\n",
    "                    r=8,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,376,384 || all params: 6,406,606,784 || trainable%: 0.24000823709676264\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , parameters in model.named_parameters():\n",
    "    if \"align\" in name:\n",
    "        parameters.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Share/home/qiyifan/.cache/huggingface/datasets/flaviagiammarino___parquet/flaviagiammarino--vqa-rad-d04980c9c3579419/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecbde06b810430e8077b75460d10da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medqa = datasets.load_dataset('flaviagiammarino/vqa-rad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = BeitImageProcessor.from_pretrained('/Share/home/qiyifan/filebase/source/beit-base-patch16-224-pt22k-ft22k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = medqa[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae2fa7015094dddb7ed8247e970eb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = train_data.map(lambda x : {'seq_x_len': len(x['question']),\n",
    "                                     \"seq_y_len\":len(x['answer'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_train(datapoint):\n",
    "    max_seq_length = 128 + 128 + 1\n",
    "    prompt_column = 'question'\n",
    "    response_column = 'answer'\n",
    "    image_column = 'image'\n",
    "    model_inputs = {\n",
    "        \"input_ids\": None,\n",
    "        \"labels\": None,\n",
    "        \"image_tensors\": None\n",
    "    }\n",
    "    if datapoint[prompt_column] and datapoint[response_column]:\n",
    "        query, answer = datapoint[prompt_column], datapoint[response_column]\n",
    "        \n",
    "        prompt = tokenizer.build_prompt(query,)\n",
    "\n",
    "        a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n",
    "                                    max_length=128)\n",
    "        b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n",
    "                                    max_length=128)\n",
    "\n",
    "        input_ids = a_ids\n",
    "        labels = b_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "        model_inputs[\"input_ids\"] =input_ids\n",
    "        model_inputs[\"labels\"] =labels\n",
    "        image_tensor=feature_extractor(datapoint[image_column], return_tensors='pt')[\"pixel_values\"].squeeze()\n",
    "        model_inputs['image_tensors'] =image_tensor\n",
    "        \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor=feature_extractor(train_data[0]['image'], return_tensors='pt')[\"pixel_values\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Share/home/qiyifan/.cache/huggingface/datasets/flaviagiammarino___parquet/flaviagiammarino--vqa-rad-d04980c9c3579419/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-626963f56ef08d75.arrow\n"
     ]
    }
   ],
   "source": [
    "train_data_prepared = train_data.map(preprocess_function_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_data_prepared[0]['image_tensors']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    len_max_batch = [len(batch[i].get(\"input_ids\")) + len(batch[i].get(\"labels\")) + 1\n",
    "                     for i in range(len(batch))]\n",
    "    len_max_batch = min(200, max(len_max_batch))\n",
    "    batch_input_ids = []\n",
    "    batch_labels = []\n",
    "    batch_image_tensors = []\n",
    "    for ba in batch:\n",
    "        x, y, image_tensor = ba.get(\"input_ids\"), ba.get(\"labels\") , ba.get('image_tensors')\n",
    "        len_padding = len_max_batch - len(x) - len(y)\n",
    "        if tokenizer.padding_side and tokenizer.padding_side == \"left\":\n",
    "            labels = [-100] * len_padding + [-100] * len(x) + y\n",
    "            input_ids = [ID_PAD] * (len_padding) + x + y\n",
    "        else:\n",
    "            labels = [-100] * len(x) + y + [-100] * len_padding\n",
    "            input_ids = x + y + [ID_PAD] * (len_padding)\n",
    "        tensor_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        tensor_labels = torch.tensor(labels, dtype=torch.long)\n",
    "        image_tensor = torch.tensor(image_tensor)\n",
    "        batch_input_ids.append(tensor_input_ids)\n",
    "        batch_labels.append(tensor_labels)\n",
    "        batch_image_tensors.append(image_tensor)\n",
    "    batch_input_ids = torch.stack(batch_input_ids)\n",
    "    batch_labels = torch.stack(batch_labels)\n",
    "    batch_image_tensors = torch.stack(batch_image_tensors)\n",
    "    input_dict = {\n",
    "                \"input_ids\": batch_input_ids,\n",
    "                \"labels\": batch_labels,\n",
    "                \"image_tensors\":batch_image_tensors,\n",
    "                }\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = data_collator(train_data_prepared.select([0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {k: v.cuda() for k, v in model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.7383, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , parameters in model.named_parameters():\n",
    "    if \"align\" in name:\n",
    "        parameters.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "filtered_state_dict = {}\n",
    "for k, v in model.named_parameters():\n",
    "    if 'lora' not in k and v.requires_grad:\n",
    "        filtered_state_dict[k] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_model.model.transformer.image_encoder.align.image_e_to_h.weight', 'base_model.model.transformer.image_encoder.align.image_e_to_h.bias', 'base_model.model.transformer.image_encoder.align.image_h_to_kv.weight', 'base_model.model.transformer.image_encoder.align.image_h_to_kv.bias'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_state_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
